{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"KuboCD","text":""},{"location":"#why-kubocd","title":"Why KuboCD ?","text":"<p>Most applications that can be deployed on Kubernetes come with a Helm chart. Moreover, this Helm chart is generally highly flexible, designed to accommodate as many contexts as possible. This can make its configuration quite complex.</p> <p>Furthermore, deploying an application on Kubernetes using an Helm Chart requires a deep understanding of the Kubernetes ecosystem. As a result, application deployment is typically the responsibility of platform administrators or platform engineers.</p> <p>And even for experienced administrators, the verbosity of Helm configurations, especially the repetition of variables, can quickly become tedious and error-prone. Therefore, industrializing these configurations is crucial to improve efficiency and reliability.</p> <p>KuboCD is a tool that enables Platform Engineers to package applications in a way that simplifies deployment for other  technical users (such as Developers, AppOps, etc.) by abstracting most of the underlying infrastructure and environment complexities.</p> <p>In addition to user applications, KuboCD can also provision core system components (e.g., ingress controllers,  load balancers, Kubernetes operators, etc.), enabling fully automated bootstrapping of a production-ready cluster  from the ground up.</p>"},{"location":"#when-to-use-kubocd","title":"When to Use KuboCD","text":"<p>KuboCD is particularly useful when:</p> <ul> <li>You want to standardize application deployment workflows across teams and environments, without requiring everyone to master Helm or Kubernetes internals.</li> <li>You are already using GitOps tools like FluxCD or ArgoCD, and need a structured way to package and manage applications as versioned, portable artifacts.</li> <li>You want to encapsulate application configuration and logic into reusable, declarative units (Packages), decoupled from cluster-specific deployment scripts.</li> <li>You need to simplify access to existing Helm charts for developers, while enforcing consistency and best practices through curated Releases.</li> <li>You want to bootstrap entire environments (including base system components like ingress controllers, operators, etc.) in a fully automated way.</li> </ul>"},{"location":"#main-concepts","title":"Main concepts","text":"<p>KuboCD introduces two core concepts that form the foundation of its deployment model:</p> <ul> <li> <p>Package:   A Package is an OCI-compliant container image that bundles an application descriptor along with one or more Helm charts.    It serves as the standardized unit of deployment, encapsulating everything needed to describe and install an application.</p> </li> <li> <p>Release   A Release is a custom Kubernetes resource that represents the deployment of a specific Package within a Kubernetes cluster.    It defines how and where the application is deployed, and manages the lifecycle of that deployment.</p> </li> </ul>"},{"location":"#kubocd-flux-helm-and-gitops","title":"KuboCD, Flux, Helm and GitOps","text":"<p>KuboCD is designed to seamlessly integrate with Flux, enabling a fully automated GitOps workflow.  While Flux handles the continuous delivery aspect (tracking changes in Git and applying them to the cluster),  KuboCD simplifies application packaging and deployment logic, making the overall delivery pipeline more modular, maintainable, and user-friendly.</p> <p>KuboCD is not a replacement for Helm. It is quite the opposite. It builds on top of Helm\u2019s proven capabilities and  leverages the rich ecosystem of existing Helm charts.</p> <p>Most production-grade applications already provide an official or community-maintained Helm chart.  KuboCD makes these charts more accessible by abstracting the complexity of Helm-based deployments.</p> <p>By encapsulating Helm charts within standardized Packages and managing them via declarative Releases,  KuboCD allows a broader audience (including less Helm-savvy users) to safely and efficiently deploy applications to Kubernetes.</p>"},{"location":"#feature-comparison-kubocd-vs-helm-vs-fluxcd","title":"Feature Comparison: KuboCD vs Helm vs FluxCD","text":"Feature / Tool KuboCD Helm FluxCD Primary Role Application packaging &amp; deployment abstraction Templating and deploying Kubernetes manifests GitOps continuous delivery User Audience Platform Engineers, AppOps, Developers DevOps, Kubernetes Experts DevOps, SREs Ease of Use High (abstracts deployment logic) Medium (requires Helm knowledge) Medium (requires GitOps understanding) Supports GitOps \u2705 (via integration with FluxCD) \u26a0\ufe0f (manual integration needed) \u2705 (native GitOps controller) Uses Helm charts \u2705 (packages &amp; manages them) \u2705 (core functionality) \u2705 (can deploy HelmReleases) Custom Resources Release, Context None (CLI and chart format) HelmRelease, Kustomization, etc. Deployment Abstraction \u2705 (encapsulates values, logic) \u274c (user-defined values needed at deploy) \u274c (relies on raw manifests or Helm) OCI Image Support \u2705 (Packages are OCI images) \u2705 (since Helm v3.8+) \u2705 (via Helm OCI support) Ideal Use Case Standardizing deployments across teams Managing complex app deployments manually Automating deployments from Git"},{"location":"tips-and-tricks/","title":"Tips and Tricks","text":""},{"location":"tips-and-tricks/#create-a-private-certificate-authority","title":"Create a private Certificate authority","text":""},{"location":"tips-and-tricks/#use-dnsmasq","title":"Use dnsmasq","text":""},{"location":"tips-and-tricks/#use-docker-mac-net-connect-and-metallb","title":"Use docker mac net connect and metallb","text":"<p>https://github.com/chipmk/docker-mac-net-connect https://medium.com/@tylerauerbeck/making-your-docker-network-reachable-in-osx-e68f998f8249</p>"},{"location":"getting-started/110-kind/","title":"Installing KuboCD on a Local Kind Cluster","text":"<p>This section walks you through setting up a local Kubernetes cluster using Docker and Kind, then installing FluxCD and KuboCD on top of it.</p> <p>Tip</p> <p>Already have a Kubernetes cluster? You can skip the cluster creation and follow the instructions in Installation on an existing cluster.</p>"},{"location":"getting-started/110-kind/#prerequisites","title":"Prerequisites","text":"<p>Ensure the following tools are installed on your workstation:</p> <ul> <li>Docker</li> <li>kubectl</li> <li>Helm</li> <li>Kind</li> <li>Flux CLI</li> </ul> <p>Make sure:</p> <ul> <li>Docker is running</li> <li>You have an active internet connection</li> <li>Ports 80 and 443 are available on your local machine</li> </ul> <p>You also need an access to an OCI-compatible container registry with permissions to push images. This is will be necessary for uploading and storing KuboCD Packages as OCI artifacts.</p>"},{"location":"getting-started/110-kind/#create-the-kind-cluster","title":"Create the Kind Cluster","text":"<p>Create a configuration file with ingress-compatible port mappings:</p> <pre><code>cat &gt;/tmp/kubodoc-config.yaml &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nname: kubodoc\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 30080\n    hostPort: 80\n    protocol: TCP\n  - containerPort: 30443\n    hostPort: 443\n    protocol: TCP\nEOF\n</code></pre> <p>Then create the cluster:</p> <pre><code>kind create cluster --config /tmp/kubodoc-config.yaml\n</code></pre> <p>This will create a single-node cluster acting as both control plane and worker node.</p> <p>Note</p> <p>The <code>extraPortMappings</code> allow direct access to services like the ingress controller from your local machine.</p> <p>Example output:</p> <pre><code>Creating cluster \"kubodoc\" ...\n \u2713 Ensuring node image (kindest/node:v1.32.2)\n \u2713 Preparing nodes\n \u2713 Writing configuration\n \u2713 Starting control-plane\n \u2713 Installing CNI\n \u2713 Installing StorageClass\nSet kubectl context to \"kind-kubodoc\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-kubodoc\n</code></pre> <p>Verify everything is running:</p> <pre><code>kubectl get pods -A\n\nNAMESPACE            NAME                                            READY   STATUS    RESTARTS   AGE\nkube-system          coredns-668d6bf9bc-nwzqj                        1/1     Running   0          52s\nkube-system          coredns-668d6bf9bc-xgv9f                        1/1     Running   0          52s\nkube-system          etcd-kubodoc-control-plane                      1/1     Running   0          59s\nkube-system          kindnet-xwfp8                                   1/1     Running   0          52s\nkube-system          kube-apiserver-kubodoc-control-plane            1/1     Running   0          59s\nkube-system          kube-controller-manager-kubodoc-control-plane   1/1     Running   0          58s\nkube-system          kube-proxy-6hv6w                                1/1     Running   0          52s\nkube-system          kube-scheduler-kubodoc-control-plane            1/1     Running   0          59s\nlocal-path-storage   local-path-provisioner-7dc846544d-k8bhb         1/1     Running   0          52s\n</code></pre>"},{"location":"getting-started/110-kind/#install-flux","title":"Install Flux","text":""},{"location":"getting-started/110-kind/#install-the-flux-cli","title":"Install the Flux CLI","text":"<p>If not already installed, follow the Flux CLI installation guide.</p>"},{"location":"getting-started/110-kind/#deploy-flux-basic-mode","title":"Deploy Flux (Basic Mode)","text":"<p>We\u2019ll begin with a basic installation of Flux (no Git repository linked for now):</p> <p>Note</p> <p>A full GitOps deployment will be described later in this documentation.</p> <pre><code>flux install\n\n\u271a generating manifests\n\u2714 manifests build completed\n\u25ba installing components in flux-system namespace\n...\n\u2714 notification-controller: deployment ready\n\u2714 source-controller: deployment ready\n\u2714 install finished\n</code></pre> <p>Verify deployment:</p> <pre><code>kubectl -n flux-system get pods\n\nNAME                                       READY   STATUS    RESTARTS   AGE\nhelm-controller-b6767d66-q27gd             1/1     Running   0          14m\nkustomize-controller-5b56686fbc-hpkhl      1/1     Running   0          14m\nnotification-controller-58ffd586f7-bbvwv   1/1     Running   0          14m\nsource-controller-6ff87cb475-hnmxv         1/1     Running   0          14m\n</code></pre> <p>Tip</p> <p>\ud83d\udca1 Want a minimal install? You can limit Flux to the required components for KuboCD: <code>flux install --components source-controller,helm-controller</code></p>"},{"location":"getting-started/110-kind/#install-kubocd","title":"Install KuboCD","text":"<p>Deploy KuboCD using Helm:</p> <pre><code>helm -n kubocd install kubocd-ctrl --create-namespace oci://quay.io/kubocd/charts/kubocd-ctrl:v0.2.0\n</code></pre>"},{"location":"getting-started/110-kind/#install-the-kubocd-cli","title":"Install the KuboCD CLI","text":"<p>Download the KuboCD CLI from the GitHub releases page. and rename it to <code>kubocd</code>. Then make it executable and move it to your path:</p> <pre><code>mv kubocd_*_* kubocd\nchmod +x kubocd\nsudo mv kubocd /usr/local/bin/\n</code></pre> <p>Verify the installation:</p> <pre><code>kubocd version\n</code></pre> <p>You can now move to your first deployment with KuboCD</p>"},{"location":"getting-started/120-existing-cluster/","title":"Installing KuboCD on an existing cluster.","text":"<p>If you have an existing cluster, you can use it to test KuboCD</p> <p>Tip</p> <p>If you don't have one, you can use a Kind cluster on your local workstation</p>"},{"location":"getting-started/120-existing-cluster/#prerequisites","title":"Prerequisites","text":"<p>Ensure the following tools are installed on your workstation:</p> <ul> <li>Docker</li> <li>kubectl</li> <li>Helm</li> <li>Flux CLI</li> </ul> <p>Make sure:</p> <ul> <li>Docker is running</li> <li>You have an active internet connection</li> <li>You have full admin rights on the target cluster.</li> </ul> <p>You also need an access to an OCI-compatible container registry with permissions to push images. This is will be necessary for uploading and storing KuboCD Packages as OCI artifacts.</p>"},{"location":"getting-started/120-existing-cluster/#install-flux","title":"Install Flux","text":""},{"location":"getting-started/120-existing-cluster/#install-the-flux-cli","title":"Install the Flux CLI","text":"<p>If not already installed, follow the Flux CLI installation guide.</p>"},{"location":"getting-started/120-existing-cluster/#deploy-flux-basic-mode","title":"Deploy Flux (Basic Mode)","text":"<p>If Flux is not already installed on your cluster, we\u2019ll begin with a basic installation (no Git repository linked for now):</p> <pre><code>flux install\n\n\u271a generating manifests\n\u2714 manifests build completed\n\u25ba installing components in flux-system namespace\n...\n\u2714 notification-controller: deployment ready\n\u2714 source-controller: deployment ready\n\u2714 install finished\n</code></pre> <p>Verify deployment:</p> <pre><code>kubectl -n flux-system get pods\n\nNAME                                       READY   STATUS    RESTARTS   AGE\nhelm-controller-b6767d66-q27gd             1/1     Running   0          14m\nkustomize-controller-5b56686fbc-hpkhl      1/1     Running   0          14m\nnotification-controller-58ffd586f7-bbvwv   1/1     Running   0          14m\nsource-controller-6ff87cb475-hnmxv         1/1     Running   0          14m\n</code></pre> <p>Tip</p> <p>\ud83d\udca1 Want a minimal install? You can limit Flux to the required components for KuboCD: <code>flux install --components source-controller,helm-controller</code></p>"},{"location":"getting-started/120-existing-cluster/#install-kubocd","title":"Install KuboCD","text":"<p>Deploy the KuboCD controller using Helm:</p> <pre><code>helm -n kubocd install kubocd-ctrl --create-namespace oci://quay.io/kubocd/charts/kubocd-ctrl:v0.2.0\n</code></pre>"},{"location":"getting-started/120-existing-cluster/#enabling-webhook-based-features","title":"Enabling Webhook-Based Features","text":"<p>Some advanced features in KuboCD such as Release protection rely on a Kubernetes validating webhook.</p> <p>To enable these features, you need to deploy a webhook component alongside the controller. This webhook requires  cert-manager to be installed in your cluster to handle TLS certificate provisioning.</p> <p>If you already have <code>cert-manager</code> installed, you can deploy the webhook with the following command:</p> <pre><code>helm -n kubocd install kubocd-wh oci://quay.io/kubocd/charts/kubocd-wh:v0.2.0\n</code></pre> <p>Note</p> <p>Don\u2019t have <code>cert-manager</code> yet? No problem \u2014 we\u2019ll show you how to package and install it with KuboCD in a later section.</p>"},{"location":"getting-started/120-existing-cluster/#install-the-kubocd-cli","title":"Install the KuboCD CLI","text":"<p>Download the KuboCD CLI from the GitHub releases page. and rename it to <code>kubocd</code>. Then make it executable and move it to your path:</p> <pre><code>mv kubocd_*_* kubocd\nchmod +x kubocd\nsudo mv kubocd /usr/local/bin/\n</code></pre> <p>Verify the installation:</p> <pre><code>kubocd version\n</code></pre> <p>You can now move to your first deployment with KuboCD</p>"},{"location":"getting-started/130-a-first-deployment/","title":"A First Deployment with KuboCD","text":""},{"location":"getting-started/130-a-first-deployment/#package-definition","title":"Package Definition","text":"<p>For this initial deployment, we\u2019ll use a simple and illustrative example: a tiny web application called podinfo.</p> <p>A Package in KuboCD is defined using a YAML manifest. Below is an example that wraps the <code>podinfo</code> application:</p> podinfo-p01.yaml <pre><code>apiVersion: v1alpha1\ntype: Package\nname: podinfo\ntag: 6.7.1-p01\nschema:\n  parameters:\n    $schema: http://json-schema.org/schema#\n    additionalProperties: false\n    properties:\n      fqdn:\n        type: string\n      ingressClassName:\n        default: nginx\n        type: string\n    required:\n      - fqdn\n    type: object\nmodules:\n  - name: main\n    specPatch:\n      timeout: 2m\n    source:\n      helmRepository:\n        url: https://stefanprodan.github.io/podinfo\n        chart: podinfo\n        version: 6.7.1\n    values: |\n      ingress:\n        enabled: true\n        className: {{ .Parameters.ingressClassName }}\n        hosts:\n          - host: {{ .Parameters.fqdn }}\n            paths:\n              - path: /\n                pathType: ImplementationSpecific\n</code></pre> <p>Note</p> <p>A KuboCD Package is not a native Kubernetes resource.</p> <p>Description of the Sample Package Attributes:</p> <ul> <li><code>apiVersion</code> (Required): Defines the version of the KuboCD Package format. The only supported value currently is <code>v1alpha1</code>.</li> <li><code>type</code>: Specifies the resource type. It must be <code>Package</code>, which is also the default and can be omitted.</li> <li><code>name</code>: The name of the package. This will also be used in the OCI image name.</li> <li><code>tag</code> (Required): Specifies the version tag of the OCI image. While technically flexible, it is recommended to follow this convention:<ul> <li>Use the Helm chart version as a base, followed by <code>-pXX</code> where <code>XX</code> denotes the packaging revision (e.g. different configurations for the same chart).</li> </ul> </li> <li><code>schema.parameters</code>: Defines input parameters for the package, using a standard OpenAPI/JSON Schema. This enables validation and documentation of parameters at deployment time.<ul> <li>If not defined, the release will not accept parameters.</li> </ul> </li> <li><code>modules</code> (Required): A package contains one or more Helm charts, each represented as a module.<ul> <li><code>modules[X].name</code> (Required): A unique name for the module. In this example, there's only one module, called <code>main</code>.</li> <li><code>modules[X].source</code> (Required): Defines where to find the Helm chart. In this example, it's in a Helm repository, but it could also come from an OCI registry, Git repository, or local chart.</li> <li><code>values</code>: This is a template rendered into a <code>values.yaml</code> for Helm. <ul> <li>The templating engine is the same as Helm\u2019s.</li> <li>The data model, however, differs. It includes a <code>.Parameters</code> object containing the values provided during deployment (via the <code>Release</code> object).</li> <li>Though it appears as YAML, it is actually a string, allowing full templating flexibility.</li> </ul> </li> </ul> </li> </ul> <p>Required Fields</p> <p>Any attribute marked with (Required) must be specified for the package to be valid.</p> <p>Tip</p> <p>More attributes and advanced features will be introduced later in the documentation.</p>"},{"location":"getting-started/130-a-first-deployment/#package-build","title":"Package Build","text":"<p>Now that the package definition is complete, it\u2019s time to generate the corresponding OCI image.</p> <p>As mentioned earlier, KuboCD uses an OCI-compatible container registry to store and distribute packages. You'll need access to one with permission to push images.</p> <p>\u2705 Supported registries: - <code>quay.io</code> (Red Hat) - <code>ghcr.io</code> (GitHub) - distribution registry</p> <p>\u26a0\ufe0f Note: Docker Hub is not supported at the moment.</p> <p>Make sure you're authenticated with the registry, e.g.:</p> <pre><code>docker login quay.io\n</code></pre> <p>Depending on the registry, the image may need to be pushed under an organization or namespace. For this example, we'll use <code>quay.io/kubodoc</code>.</p> <p>To build and push the package image:</p> <pre><code>kubocd package podinfo-p01.yaml --ociRepoPrefix quay.io/kubodoc/packages\n</code></pre> <p>Note</p> <p>Adjust <code>--ociRepoPrefix</code> to your own registry setup. The <code>packages</code> suffix is arbitrary \u2014 you can use any subpath or omit it.</p> <p>The resulting repository and tag are determined from the package manifest:</p> <ul> <li>Repository = <code>--ociRepoPrefix</code> + package <code>name</code></li> <li>Tag = package <code>tag</code></li> </ul> <p>Expected Output:</p> <pre><code>====================================== Packaging package 'podinfo-p01.yaml'\n--- Handling module 'main':\nFetching chart podinfo:6.7.1...\nChart: podinfo:6.7.1\n--- Packaging\nGenerating index file\nWrap all in assembly.tgz\n--- push OCI image: quay.io/kubodoc/packages/podinfo:6.7.1-p01\nSuccessfully pushed\n</code></pre> <p>You can also set the repository prefix globally via an environment variable:</p> <pre><code>export OCI_REPO_PREFIX=quay.io/kubodoc/packages\n</code></pre> <p>Or for other registries:</p> <pre><code>export OCI_REPO_PREFIX=ghcr.io/kubodoc/packages\nexport OCI_REPO_PREFIX=localhost:5000/packages\n</code></pre> <p>Warning</p> <p>By default, pushed images may be private. To make them accessible for deployment, ensure the image is set to public.</p> <p>Note</p> <p>If you prefer to keep the image private, you will need to provide authentication credentials in the <code>Release</code> configuration. This is explained later in the documentation.</p>"},{"location":"getting-started/130-a-first-deployment/#releasing-the-application","title":"Releasing the Application","text":"<p>To deploy the application, define a KuboCD <code>Release</code> custom resource:</p> podinfo-basic.yaml <pre><code>---\napiVersion: kubocd.kubotal.io/v1alpha1\nkind: Release\nmetadata:\n  name: podinfo1\n  namespace: default\nspec:\n  description: A first sample release of podinfo\n  package:\n    repository: quay.io/kubodoc/packages/podinfo\n    tag: 6.7.1-p01\n    interval: 30m\n  parameters:\n    fqdn: podinfo1.ingress.kubodoc.local\n</code></pre> <p>Explanation of Key Attributes:</p> <ul> <li><code>description</code>: (Optional) A short description of this release.</li> <li><code>package.repository</code>: The OCI image repository that contains the package. This should match the registry used during package build.</li> <li><code>package.tag</code>: The image tag, which should match the one defined in the package manifest.</li> <li><code>package.interval</code>: Specifies how frequently KuboCD checks the registry for updates to the image.</li> <li><code>parameters</code>: The values required by the package schema. In this example, only a single parameter (<code>fqdn</code>) is needed.</li> </ul> <p>Deploying the Application:</p> <ol> <li>Adjust the repository and parameters (if needed) to match your environment.</li> <li>Apply the Release:</li> </ol> <pre><code>kubectl create -f podinfo-basic.yaml\n\nrelease.kubocd.kubotal.io/podinfo1 created\n</code></pre> <p>Once deployed, monitor the status:</p> <pre><code>kubectl get releases\n\nNAME       REPOSITORY                         TAG         CONTEXTS   STATUS   READY   WAIT   PRT   AGE     DESCRIPTION\npodinfo1   quay.io/kubodoc/packages/podinfo   6.7.1-p01              READY    1/1            -     6m40s   A first sample release of podinfo\n</code></pre> <p>You can also verify the pod:</p> <pre><code>kubectl get pods\n\nNAME                             READY   STATUS    RESTARTS   AGE\npodinfo1-main-779b6b9fd4-zbgbx   1/1     Running   0          8h\n</code></pre>"},{"location":"getting-started/140-under-the-hood/","title":"Under the Hood (If Things Go Wrong)","text":"<p>Behind the scenes, KuboCD creates several FluxCD resources to manage the deployment.</p> <p>You can inspect these resources to debug problems or understand the internals.</p> <pre><code>kubectl describe release podinfo1\n......\nEvents:\nType    Reason                 Age   From     Message\n----    ------                 ----  ----     -------\nNormal  OCIRepositoryCreated   12s   release  Created OCIRepository \"kcd-podinfo1\"\nNormal  HelmRepositoryCreated  10s   release  Created HelmRepository \"kcd-podinfo1\"\nNormal  HelmReleaseCreated     8s    release  Created HelmRelease \"podinfo1-main\"\n</code></pre> <p>All of these resources are created in the same namespace as the <code>Release</code> object (e.g. <code>default</code>).</p>"},{"location":"getting-started/140-under-the-hood/#the-ocirepository","title":"The OCIRepository","text":"<p>This Flux resource pulls the KuboCD package image:</p> <pre><code>kubectl get OCIRepository\n\nNAME           URL                                      READY   STATUS                                                                                                           AGE\nkcd-podinfo1   oci://quay.io/kubodoc/packages/podinfo   True    stored artifact for digest '6.7.1-p01@sha256:985e4e2f89a4b17bd5cc2936a0b305df914ae479e0b8c96e61cb22725b61cd24'   9m1s\n</code></pre> <p>Tip</p> <p>If the release is stuck in the <code>WAIT_OCI</code> state, check this resource. Common issues include:</p> <ul> <li>Incorrect URL</li> <li>Image still private (set to public or provide authentication)</li> </ul> <p>You can manually delete this resource to trigger a refresh:</p> <pre><code>kubectl delete ocirepository kcd-podinfo1\n</code></pre> <p>KuboCD will recreate it.</p> <p>This can be useful to force a relaod of a modified OCI image.</p>"},{"location":"getting-started/140-under-the-hood/#the-helmrepository","title":"The HelmRepository","text":"<p>As the <code>podinfo</code> Helm the chart is embedded in the package, it must be served to Flux via an internal Helm repository.</p> <p>KuboCD creates a <code>HelmRepository</code> resource pointing to its internal server:</p> <pre><code>kubectl get HelmRepository\n\nNAME           URL                                                                            AGE    READY   STATUS\nkcd-podinfo1   http://kubocd-ctrl-controller-helm-repository.kubocd.svc/hr/default/podinfo1   105m   True    stored artifact: revision 'sha256:d8db03cf45ecd75064c2a2582812dc4df5cd624d0e295b24ff79569bf46a070b'\n</code></pre> <p>This step rarely causes errors unless the internal controller is unreachable.</p>"},{"location":"getting-started/140-under-the-hood/#the-helmrelease","title":"The HelmRelease","text":"<p>This Flux resource handles the actual Helm chart deployment.</p> <pre><code>kubectl get HelmRelease\n\nNAME            AGE     READY   STATUS\npodinfo1-main   7m59s   True    Helm install succeeded for release default/podinfo1-main.v1 with chart podinfo@6.7.1\n</code></pre> <p>Note</p> <p>There will be one HelmRelease per module in the package.</p> <p>If the release is stuck in <code>WAIT_HREL</code>, inspect this resource:</p> <pre><code>kubectl describe helmrelease podinfo1-main\n\n.....\nEvents:\n  Type    Reason            Age    From             Message\n  ----    ------            ----   ----             -------\n  Normal  HelmChartCreated  5m42s  helm-controller  Created HelmChart/default/default-podinfo1-main with SourceRef 'HelmRepository/default/kcd-podinfo1'\n  Normal  InstallSucceeded  5m39s  helm-controller  Helm install succeeded for release default/podinfo1-main.v1 with chart podinfo@6.7.1\n</code></pre> <p>One of the point to check if the generated <code>values</code> for the Helm chart deployment.</p> <p>Note</p> <p>You will need to unstall the yq command</p> <pre><code>kubectl get HelmRelease podinfo1-main -o yaml | yq '.spec.values'\n\ningress:\n  className: nginx\n  enabled: true\n  hosts:\n    - host: podinfo1.ingress.kubodoc.local\n      paths:\n        - path: /\n          pathType: ImplementationSpecific\n</code></pre> <p>Note</p> <p>If deployment fails, you may need to wait for the Helm timeout to expire (default: 2 minutes). You can configure this value in the package or in the Release.</p>"},{"location":"getting-started/150-ingress-controller/","title":"Setting Up the Ingress Controller","text":"<p>Warning</p> <p>If you're using an existing cluster, there's likely already an ingress controller installed. Do not install another one. However, you should still read this section to understand how ingress integration works in KuboCD.</p> <p>If you're following the local <code>kind</code> cluster setup, you\u2019ll need to install an ingress controller to make use of the deployed <code>Ingress</code> object.</p> <p>Check the current <code>Ingress</code> object created by the <code>podinfo</code> deployment:</p> <pre><code>kubectl get ingresses\n\nNAME            CLASS   HOSTS                            ADDRESS   PORTS   AGE\npodinfo1-main   nginx   podinfo1.ingress.kubodoc.local             80      6m33s\n</code></pre> <p>At this point, the ingress is inactive since no ingress controller is installed.</p>"},{"location":"getting-started/150-ingress-controller/#build-the-ingress-nginx-package","title":"Build the ingress-nginx package:","text":"<p>Here is a sample package definition for deploying the <code>ingress-nginx</code> controller:</p> <p>Warning</p> <p>This first version is dedicated to the way we setup the cluster, using the kind portMapping and NodePorts</p> ingress-nginx-p01.yaml <pre><code>apiVersion: v1alpha1\nname: ingress-nginx\ntag: 4.12.1-p01\nprotected: true\nmodules:\n  - name: main\n    specPatch:\n      timeout: 4m\n    source:\n      helmRepository:\n        url: https://kubernetes.github.io/ingress-nginx\n        chart: ingress-nginx\n        version: 4.12.1\n    values:\n      controller:\n        extraArgs:\n          enable-ssl-passthrough: true\n        service:\n          type: NodePort\n          nodePorts:\n            http: \"30080\"\n            https: \"30443\"\nroles:\n  - ingress\n</code></pre> <p>New key points compared to the <code>podinfo</code> Package:</p> <ul> <li><code>protected: true</code>: Prevents accidental deletion of the release. (Currently not enforced unless KuboCD webhook is installed.)</li> <li><code>timeout: 4m</code>: Overrides the default deployment timeout (<code>2m</code>) because this Helm chart may take some time to deploy.</li> <li><code>values</code>: This section is in proper YAML format (not a templated string), since it does not include any templating.</li> <li><code>roles</code>: Assigns the package to the <code>ingress</code> role. This is used for dependency management between releases.</li> </ul> <p>Build the package:</p> <pre><code>kubocd pack ingress-nginx-p01.yaml\n\n====================================== Packaging package 'ingress-nginx.yaml'\n--- Handling module 'main':\nFetching chart ingress-nginx:4.12.1...\nChart: ingress-nginx:4.12.1\n--- Packaging\nGenerating index file\nWrap all in assembly.tgz\n--- push OCI image: quay.io/kubodoc/packages/ingress-nginx:4.12.1-p01\nSuccessfully pushed\n</code></pre>"},{"location":"getting-started/150-ingress-controller/#deploy-the-package","title":"Deploy the package","text":"<p>Then define the <code>Release</code> resource:</p> ingress-nginx.yaml <pre><code>---\napiVersion: kubocd.kubotal.io/v1alpha1\nkind: Release\nmetadata:\n  name: ingress-nginx\n  namespace: kubocd\nspec:\n  description: The Ingress controller\n  protected: false\n  package:\n    repository: quay.io/kubodoc/packages/ingress-nginx\n    tag: 4.12.1-p01\n    interval: 30m\n  targetNamespace: ingress-nginx\n  createNamespace: true\n</code></pre> <p>Key Points:</p> <ul> <li><code>metadata.namespace: kubocd</code>: As it is a system components, deploy in a restricted namespace.</li> <li><code>spec.protected: false</code>: Demonstrates that the package-level <code>protected</code> flag can be overridden at the release level.</li> <li><code>spec.targetNamespace: ingress-nginx</code>: Installs the ingress controller in its own namespace.</li> <li><code>spec.createNamespace: true</code>: Automatically creates the target namespace if it doesn't exist.</li> </ul> <p>Apply the release:</p> <pre><code>kubectl create -f ingress-nginx.yaml\n\nrelease.kubocd.kubotal.io/ingress-nginx created\n</code></pre> <p>Check the release status:</p> <pre><code>kubectl -n kubocd get release\n\nNAME            REPOSITORY                               TAG          CONTEXTS   STATUS   READY   WAIT   PRT   AGE   DESCRIPTION\ningress-nginx   quay.io/kubodoc/packages/ingress-nginx   4.12.1-p01              READY    1/1            -     86s   The Ingress controller\n</code></pre>"},{"location":"getting-started/150-ingress-controller/#configure-the-dns-entry","title":"Configure the DNS entry","text":"<p>To access the <code>podinfo</code> application, you'll need to define a DNS entry matching the <code>fqdn</code> parameter.</p> <p>The simplest way in our case is to use the <code>/etc/hosts</code> file:</p> <pre><code>127.0.0.1 localhost podinfo1.ingress.kubodoc.local\n</code></pre> <p>Make sure the hostname matches exactly what was provided in the Release parameters.</p> <p>You should now be able to access the 'podinfo` web server:</p> <p>\ud83d\udc49 http://podinfo1.ingress.kubodoc.local</p>"},{"location":"getting-started/160-the-context/","title":"The Context","text":"<p>One of the key features of KuboCD is its ability to generate Helm deployment values files from a small set of high-level input parameters, using a templating mechanism.</p> <p>This mechanism combines a template with a data model.</p> <p>Our first example uses only the <code>.Parameters</code> element of the data model:</p> podinfo-p01.yaml <pre><code>apiVersion: v1alpha1\ntype: Package\nname: podinfo\n...\nmodules:\n  - name: main\n    ...\n    values: |\n      ingress:\n        enabled: true\n        className: {{ .Parameters.ingressClassName }}\n        hosts:\n          - host: {{ .Parameters.fqdn }}\n            paths:\n              - path: /\n                pathType: ImplementationSpecific\n</code></pre> <p>In fact, the data model includes the following top-level elements:</p> <ul> <li><code>.Parameters</code>: The parameters provided in the <code>Release</code> custom resource.</li> <li><code>.Release</code>: The release object itself.</li> <li><code>.Context</code>: The deployment context.</li> </ul> <p>The context is a YAML object with a flexible structure, designed to hold shared configuration data relevant to all deployments.</p> <p>For example, the <code>podinfo</code> package includes a parameter <code>ingressClassName</code> with a default value (<code>nginx</code>). If a cluster uses a different ingress controller, this value would need to be overridden for all relevant <code>Release</code> objects.</p> <p>This type of shared configuration is best defined in a global cluster-level context.</p> <p>Similarly, if all application ingress URLs share a common root domain, that too should be centralized.</p> <p>Here's an initial example of how this logic can be implemented.</p>"},{"location":"getting-started/160-the-context/#context-creation","title":"Context Creation","text":"<p>A <code>Context</code> is a KuboCD resource:</p> cluster.yaml <pre><code>apiVersion: kubocd.kubotal.io/v1alpha1\nkind: Context\nmetadata:\n  namespace: contexts\n  name: cluster\nspec:\n  description: Global context for the kubodoc cluster\n  protected: true\n  context:\n    ingress:\n      className: nginx\n      domain: ingress.kubodoc.local\n    storageClass: \n      data: standard\n      workspace: standard\n</code></pre> <p>Key attributes:</p> <ul> <li><code>description</code>: A short description.</li> <li><code>protected</code>: Prevents deletion of this object. Requires KuboCD's webhook feature.</li> <li><code>context</code>: A tree of values that is injected into the data model for the deployment parameter templates. This section:<ul> <li>Must be valid YAML.</li> <li>Has a flexible structure, but should align with what the package templates expect.</li> </ul> </li> </ul> <p>In this example, the context includes:</p> <ul> <li><code>ingress.className</code>: The ingress controller type.</li> <li><code>ingress.domain</code>: The suffix used for building ingress URLs.</li> <li><code>storageClass</code>: Two Kubernetes <code>StorageClass</code> definitions for different application profiles. For our <code>kind</code> cluster, there is only one available option: <code>standard</code>.</li> </ul> <p>Contexts should be placed in a dedicated namespace:</p> <pre><code>kubectl create ns contexts\n</code></pre> <pre><code>kubectl create -f cluster.yaml\n</code></pre> <p>Note</p> <p>Since the context is shared among most of all applications, its structure must be carefully designed and well documented.</p>"},{"location":"getting-started/160-the-context/#package-adaptation","title":"Package Adaptation","text":"<p>Our initial <code>podinfo</code> package did not account for the context concept. Here is an updated version:</p> podinfo-p02.yaml <pre><code>apiVersion: v1alpha1\ntype: Package\nname: podinfo\ntag: 6.7.1-p02\nschema:\n  parameters:\n    $schema: http://json-schema.org/schema#\n    type: object\n    additionalProperties: false\n    properties:\n      host: { type: string }\n    required:\n      - host\n  context:\n    $schema: http://json-schema.org/schema#\n    additionalProperties: true\n    type: object\n    properties:\n      ingress:\n        type: object\n        additionalProperties: true\n        properties:\n          className: { type: string }\n          domain: { type: string }\n        required:\n          - domain\n          - className\n    required:\n      - ingress\nmodules:\n  - name: main\n    specPatch:\n      timeout: 2m\n    source:\n      helmRepository:\n        url: https://stefanprodan.github.io/podinfo\n        chart: podinfo\n        version: 6.7.1\n    values: |\n      ingress:\n        enabled: true\n        className: {{ .Context.ingress.className  }}\n        hosts:\n          - host: {{ .Parameters.host }}.{{ .Context.ingress.domain }}\n            paths:\n              - path: /\n                pathType: ImplementationSpecific\n</code></pre> <p>Key points:</p> <ul> <li>The <code>tag</code> was updated to generate a new version.</li> <li>The <code>fqdn</code> parameter was replaced with <code>host</code> to represent only the hostname (excluding the domain).</li> <li>The <code>modules[X].values</code> section now uses the context.</li> <li>A <code>schema.context</code> section has been added to define and validate the expected context structure.</li> </ul> <p>This new version must be packaged:</p> <pre><code>kubocd pack podinfo-p02.yaml\n\n====================================== Packaging package 'podinfo-p02.yaml'\n--- Handling module 'main':\n    Fetching chart podinfo:6.7.1...\n    Chart: podinfo:6.7.1\n--- Packaging\n    Generating index file\n    Wrap all in assembly.tgz\n--- push OCI image: quay.io/kubodoc/packages/podinfo:6.7.1-p02\n    Successfully pushed\n</code></pre>"},{"location":"getting-started/160-the-context/#deployment","title":"Deployment","text":"<p>Here is the corresponding deployment manifest:</p> podinfo-ctx.yaml <pre><code>---\napiVersion: kubocd.kubotal.io/v1alpha1\nkind: Release\nmetadata:\n  name: podinfo2\n  namespace: default\nspec:\n  description: A first sample release of podinfo\n  package:\n    repository: quay.io/kubodoc/packages/podinfo\n    tag: 6.7.1-p02\n    interval: 30m\n  parameters:\n    host: podinfo2\n  contexts:\n    - namespace: contexts\n      name: cluster\n</code></pre> <p>Key points:</p> <ul> <li>The <code>fqdn</code> parameter was replaced with <code>host</code>.</li> <li>A new <code>spec.contexts</code> section lists the contexts to merge into a single object passed to the template engine.</li> </ul> <p>Warning</p> <p>Referencing a non-existent context results in an error.</p> <p>Once <code>spec.repository</code> is set correctly, apply the deployment:</p> <pre><code>kubectl create -f podinfo-ctx.yaml\n</code></pre> <p>Check that the new <code>Release</code> reaches the <code>READY</code> state:</p> <pre><code>kubectl get releases podinfo2\n\nNAME       REPOSITORY                         TAG         CONTEXTS           STATUS   READY   WAIT   PRT   AGE   DESCRIPTION\npodinfo2   quay.io/kubodoc/packages/podinfo   6.7.1-p02   contexts:cluster   READY    1/1            -     17m   A first sample release of podinfo\n</code></pre>"},{"location":"getting-started/160-the-context/#context-aggregation","title":"Context Aggregation","text":"<p>An application's effective context may result from the aggregation of multiple context objects.</p> <p>For instance, a project-level context can be created to share variables across all applications within a project. This will be merged with the global cluster context.</p> <p>In the following examples, each deployed project has its own namespace and context.</p>"},{"location":"getting-started/160-the-context/#example-1-context-addition","title":"Example 1: Context addition","text":"<p>Create the namespace:</p> <pre><code>kubectl create namespace project01\n</code></pre> <p>Then create the project context:</p> project01.yaml <pre><code>apiVersion: kubocd.kubotal.io/v1alpha1\nkind: Context\nmetadata:\n  name: project01\nspec:\n  description: Context for project 1\n  context:\n    project:\n      id: p01\n      subdomain: prj01\n</code></pre> <p>Note that the <code>namespace</code> is not specified in the manifest. It will be set via the command line:</p> <pre><code>kubectl -n project01 create -f project01.yaml\n</code></pre> <p>List all defined contexts:</p> <pre><code>kubectl get --all-namespaces contexts.kubocd.kubotal.io\n\nNAMESPACE   NAME        DESCRIPTION                          PARENTS   STATUS   AGE\ncontexts    cluster     Global context for the kubodoc cluster             READY    2d2h\nproject01   project01   Context for project 1                            READY    2m35s\n</code></pre> <p>This example requires modifying the package to include the new variable in the <code>values</code> template and in the <code>schema.context</code> section:</p> podinfo-p03.yaml <pre><code>apiVersion: v1alpha1\ntype: Package\nname: podinfo\ntag: 6.7.1-p03\nschema:\n  parameters:\n    $schema: http://json-schema.org/schema#\n    type: object\n    additionalProperties: false\n    properties:\n      host: { type: string }\n    required:\n      - host\n  context:\n    $schema: http://json-schema.org/schema#\n    additionalProperties: true\n    type: object\n    properties:\n      ingress:\n        type: object\n        additionalProperties: true\n        properties:\n          className: { type: string }\n          domain: { type: string }\n        required:\n          - domain\n          - className\n      project:\n        type: object\n        additionalProperties: true\n        properties:\n          subdomain: { type: string }\n        required:\n          - subdomain\n    required:\n      - ingress\n      - project\nmodules:\n  - name: main\n    specPatch:\n      timeout: 2m\n    source:\n      helmRepository:\n        url: https://stefanprodan.github.io/podinfo\n        chart: podinfo\n        version: 6.7.1\n    values: |\n      ingress:\n        enabled: true\n        className: {{ .Context.ingress.className  }}\n        hosts:\n          - host: {{ .Parameters.host }}.{{ .Context.project.subdomain }}.{{ .Context.ingress.domain }}\n            paths:\n              - path: /\n                pathType: ImplementationSpecific\n</code></pre> <p>Don't forget to package it:</p> <pre><code>kubocd pack podinfo-p03.yaml\n</code></pre> <p>Create a new <code>Release</code> for deployment:</p> podinfo-prj01.yaml <pre><code>---\napiVersion: kubocd.kubotal.io/v1alpha1\nkind: Release\nmetadata:\n  name: podinfo\nspec:\n  description: A release of podinfo on project01\n  package:\n    repository: quay.io/kubodoc/packages/podinfo\n    tag: 6.7.1-p03\n    interval: 30m\n  parameters:\n    host: podinfo\n  contexts:\n    - namespace: contexts\n      name: cluster\n    - name: project01\n  debug:\n    dumpContext: true\n    dumpParameters: true\n</code></pre> <p>Notes:</p> <ul> <li><code>metadata.namespace</code> is not defined; it will be set via command line.</li> <li><code>metadata.name</code> is simply <code>podinfo</code>, assuming only one instance per namespace.</li> <li><code>spec.contexts</code> includes two entries, with the second referencing the project context.</li> <li>A <code>debug</code> section is added to include the merged context and parameters in the <code>Release</code> status.</li> </ul> <p>Deploy the release:</p> <pre><code>kubectl -n project01 create -f podinfo-prj01.yaml\n</code></pre> <p>Verify both contexts are listed:</p> <pre><code>kubectl -n project01 get releases podinfo\n\nNAME      REPOSITORY                         TAG         CONTEXTS                               STATUS   READY   WAIT   PRT   AGE     DESCRIPTION\npodinfo   quay.io/kubodoc/packages/podinfo   6.7.1-p03   contexts:cluster,project01:project01   READY    1/1            -     8m31s   A release of podinfo on project01\n</code></pre> <p>Check the resulting ingress object:</p> <pre><code>kubectl get --all-namespaces ingress\n\nNAMESPACE   NAME            CLASS   HOSTS                                  ADDRESS        PORTS   AGE\ndefault     podinfo1-main   nginx   podinfo1.ingress.kubodoc.local         10.96.218.98   80      2d20h\ndefault     podinfo2-main   nginx   podinfo2.ingress.kubodoc.local         10.96.218.98   80      71m\nproject01   podinfo-main    nginx   podinfo.prj01.ingress.kubodoc.local    10.96.218.98   80      13m\n</code></pre> <p>Inspect the <code>Release</code> status:</p> <pre><code>kubectl -n project01 get release podinfo -o yaml\n\napiVersion: kubocd.kubotal.io/v1alpha1\nkind: Release\nmetadata:\n  ....\nspec:\n  ....\nstatus:\n  context:\n    ingress:\n      className: nginx\n      domain: ingress.kubodoc.local\n    project:\n      id: p01\n      subdomain: prj01\n    storageClass:\n      data: standard\n      workspace: standard\n  ....      \n  parameters:\n    host: podinfo2\n  ....\n</code></pre> <p>The merged context includes values from both the cluster and project contexts.</p> <p>Warning</p> <p>In real-world scenarios, the context may become quite large. Use this debug mode sparingly.</p>"},{"location":"getting-started/160-the-context/#example-2-context-override","title":"Example 2: Context override","text":"<p>In this second example, the objective remains the same (adding a subdomain to the ingress), but we use the initial version of the package that only leverages the cluster context.</p> <p>Create a dedicated namespace:</p> <pre><code>kubectl create ns project02\n</code></pre> <p>Create a project context in that namespace:</p> project02.yaml <pre><code>apiVersion: kubocd.kubotal.io/v1alpha1\nkind: Context\nmetadata:\n  name: project02\nspec:\n  description: Context for project 2\n  context:\n    project:\n      id: p02\n    ingress:\n      domain: prj02.ingress.kubodoc.local\n</code></pre> <pre><code>kubectl -n project02 create -f project02.yaml\n</code></pre> <p>Note that the same <code>spec.context.ingress.domain</code> path exists in both the project and cluster contexts. When contexts are merged in a <code>Release</code>, later contexts in the list override earlier ones. Thus, the project\u2019s value takes precedence.</p> podinfo-prj02.yaml <pre><code>---\napiVersion: kubocd.kubotal.io/v1alpha1\nkind: Release\nmetadata:\n  name: podinfo\nspec:\n  description: A release of podinfo on project02\n  package:\n    repository: quay.io/kubodoc/packages/podinfo\n    tag: 6.7.1-p02\n    interval: 30m\n  parameters:\n    host: podinfo\n  contexts:\n    - namespace: contexts\n      name: cluster\n    - name: project02\n  debug:\n    dumpContext: true\n    dumpParameters: true\n</code></pre> <pre><code>kubectl -n project02 create -f podinfo-prj02.yaml\n</code></pre> <p>Check the resulting context in the <code>Release</code> object:</p> <pre><code>kubectl -n project02 get release podinfo -o yaml\n\napiVersion: kubocd.kubotal.io/v1alpha1\nkind: Release\nmetadata:\n    ....\nspec:\n    ....\nstatus:\n  context:\n    ingress:\n      className: nginx\n      domain: prj02.ingress.kubodoc.local\n    project:\n      id: p02\n    storageClass:\n      data: standard\n      workspace: standard\n  ....\n</code></pre> <p>Ensure the correct ingress host is used:</p> <pre><code>kubectl get --all-namespaces ingress\n\nNAMESPACE   NAME            CLASS   HOSTS                                 ADDRESS        PORTS   AGE\ndefault     podinfo1-main   nginx   podinfo1.ingress.kubodoc.local        10.96.218.98   80      2d20h\ndefault     podinfo2-main   nginx   podinfo2.ingress.kubodoc.local        10.96.218.98   80      110m\nproject01   podinfo-main    nginx   podinfo.prj01.ingress.kubodoc.local   10.96.218.98   80      26m\nproject02   podinfo-main    nginx   podinfo.prj02.ingress.kubodoc.local   10.96.218.98   80      2m52s\n</code></pre>"},{"location":"getting-started/160-the-context/#context-change","title":"Context Change","text":"<p>Any change to a context is automatically applied to all associated <code>Release</code> objects. However, only the deployments that are actually affected will be updated.</p> <p>Notes</p> <p>Technically, KuboCD patches the corresponding Flux <code>helmRelease</code> objects, which triggers a <code>helm upgrade</code>. This should only update the resources that are truly impacted.</p> <p>For example, modify the context for <code>project01</code>:</p> <pre><code>kubectl -n project01 patch context.kubocd.kubotal.io project01 --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/context/project/subdomain\", \"value\": \"project01\" }]'\n</code></pre> <p>Observe that the ingress is quickly updated accordingly:</p> <pre><code>kubectl get --all-namespaces ingress\n\nNAMESPACE   NAME            CLASS   HOSTS                                     ADDRESS        PORTS   AGE\ndefault     podinfo1-main   nginx   podinfo1.ingress.kubodoc.local            10.96.218.98   80      3d3h\ndefault     podinfo2-main   nginx   podinfo2.ingress.kubodoc.local            10.96.218.98   80      8h\nproject01   podinfo-main    nginx   podinfo.project01.ingress.kubodoc.local   10.96.218.98   80      7h13m\nproject02   podinfo-main    nginx   podinfo.prj02.ingress.kubodoc.local       10.96.218.98   80      6h49m\n</code></pre> <p>To restore the original value:</p> <pre><code>kubectl -n project01 patch context.kubocd.kubotal.io project01 --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/context/project/subdomain\", \"value\": \"prj01\" }]'\n</code></pre>"},{"location":"user-guide/alternate-schema-format/","title":"Alternate KuboCD Schema Format","text":"<p>In our first example, we defined <code>schema.parameters</code> using a standard OpenAPI/JSON schema. While fully supported, it can be quite verbose.</p> <p>KuboCD also supports a more concise schema syntax, specifically designed for this use case.</p> <p>Here\u2019s an identical version of the <code>podinfo</code> package using the simplified KuboCD schema format:</p> podinfo-p02.yaml <pre><code>apiVersion: v1alpha1\ntype: Package\nname: podinfo\ntag: 6.7.1-p01\nschema:\n  parameters:\n    properties:\n      fqdn: { type: string, required: true }\n      ingressClassName: { type: string, default: \"nginx\" }\nmodules:\n  - name: main\n    source:\n      helmRepository:\n        url: https://stefanprodan.github.io/podinfo\n        chart: podinfo\n        version: 6.7.1\n    values: |\n      ingress:\n        enabled: true\n        className: {{ .Parameters.ingressClassName }}\n        hosts:\n          - host: {{ .Parameters.fqdn }}\n            paths:\n              - path: /\n                pathType: ImplementationSpecific\n</code></pre> <p>As you can see, the format is significantly more compact.</p> <p>Info</p> <p>This schema format is not standard OpenAPI, but KuboCD internally converts it into a compliant schema. It is a shorthand designed for convenience.</p> <p>The presence or absence of the <code>$schema:</code> key is what KuboCD uses to distinguish between standard and KuboCD schema formats.</p>"},{"location":"user-guide/cli/","title":"The KuboCD CLI","text":""}]}